# -*- coding: utf-8 -*-
"""Faceexpressionrecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ck87hO4IcPKpndF8LOmrVNHDbD__RSaI
"""

from  google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/facial_expression/

!ls

import tarfile
fname = 'fer2013.tar.gz'

if fname.endswith("tar.gz"):
   tar=tarfile.open(fname,"r:gz")
   tar.extractall()
   tar.close()
elif fname.endswith("tar"):
   tar = tarfile.open(fname,"r:")
   tar.extractall()
   tar.close()

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from matplotlib import pyplot
from sklearn.model_selection import train_test_split

df = pd.read_csv('fer2013/fer2013.csv')
df.head()

df.emotion.unique()

label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}

np.array(df.pixels.loc[0].split(' ')).reshape(48,48)

fig = pyplot.figure(1, (14,14))
k = 0
for label in sorted(df.emotion.unique()):
    for j in range(3):
        px = df[df.emotion==label].pixels.iloc[k]
        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')
        k += 1
        ax = pyplot.subplot(7, 7, k)
        ax.imshow(px)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(label_to_text[label])
        pyplot.tight_layout()

img_array = df.pixels.apply(lambda x :np.array(x.split(' ')).reshape(48, 48,1).astype('float32'))

img_array=np.stack(img_array,axis=0)

lables= df.emotion.values

X_train,X_test,y_train,y_test = train_test_split(img_array,lables,test_size =.1)

X_train.shape,y_train.shape,X_test.shape,y_test.shape

X_train=X_train/255
X_test=X_test/255

basemodel=tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(48,48,1)),
                                      tf.keras.layers.MaxPool2D(2,2),
                                      tf.keras.layers.BatchNormalization(),
                                      #
                                      tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(48,48,1)),
                                      tf.keras.layers.MaxPool2D(2,2),
                                      tf.keras.layers.BatchNormalization(),
                                      #
                                      tf.keras.layers.Conv2D(128,(3,3),activation='relu',input_shape=(48,48,1)),
                                      tf.keras.layers.MaxPool2D(2,2),
                                      tf.keras.layers.BatchNormalization(),
                                      #
                                      tf.keras.layers.Conv2D(256,(3,3),activation='relu',input_shape=(48,48,1)),
                                      tf.keras.layers.MaxPool2D(2,2),
                                      tf.keras.layers.BatchNormalization(),
                                      tf.keras.layers.Flatten(),
                                      tf.keras.layers.Dense(512,activation='relu'),
                                      tf.keras.layers.Dense(7,activation='softmax')
                                      
                                      
                                      ])

basemodel.summary()

basemodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=.0001),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

import os 
try:
  os.mkdir('checkpoint')
except:
  pass

file_name='best_model.h5'
checkpoint_path= os.path.join('checkpoint',file_name)

call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, 
                                                 monitor='val_accuracy', 
                                                 verbose=1,
                                                 save_freq='epoch',
                                                 save_best_only=True, 
                                                 save_weights_only=False, 
                                                 mode='max')

basemodel.fit(X_train,y_train,epochs=20,validation_split=0.3,callbacks=call_back)

import cv2
import numpy as np
import tensorflow
from google.colab.patches import cv2_imshow
# Load the pre-trained model
model = tensorflow.keras.models.load_model("/content/sample_data/best_model.h5")
print(model.input_shape)
# Load the face detector
face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# Load the group photo
img = cv2.imread("color.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Detect faces in the photo
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)
expression_dict = {0: "Anger", 1: "Disgust", 2: "Fear", 3: "Happiness", 4: "sadness", 5: "surprised", 6: "neutral"}

# Iterate through each face
for (x, y, w, h) in faces:
    # Align the face
    img_aligned = img[y:y+h, x:x+w]
    
    # Resize the face to 48x48 pixels
    img_aligned = cv2.resize(img_aligned, (48, 48))
    # Normalize the image
    img_aligned = img_aligned.astype('float32') / 255
    img_aligned = cv2.cvtColor(img_aligned, cv2.COLOR_RGB2GRAY)
    img_aligned = np.expand_dims(img_aligned, axis=-1)
    
    # Add an extra dimension at the beginning
    img_aligned = np.expand_dims(img_aligned, axis=0)
    print(img_aligned.shape)
    # Predict the facial expression
    predictions = model.predict(img_aligned)[0]
    label = np.argmax(predictions)
    expression = expression_dict[label]

    # Draw the facial expression label on the image
    cv2.putText(img, expression, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
cv2_imshow( img)
cv2.waitKey(0)
cv2.destroyAllWindows()